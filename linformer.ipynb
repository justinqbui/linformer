{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "linformer.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMpHAvaySX1d4nJH6TWugFE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/justinqbui/min_linformer/blob/main/linformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SHcMAJnk3pb7"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "from math import sqrt\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "!pip install pytorch_lightning\n",
        "import pytorch_lightning as pl\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install einops\n",
        "from einops import rearrange"
      ],
      "metadata": {
        "id": "oc8kfKV8sUn1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def scaled_attention(queries, keys, values):\n",
        "    d = queries.shape[-1]\n",
        "    scores = torch.matmul(queries, keys.transpose(-2,-1))/sqrt(d) #  dot product attention\n",
        "    attention_weights = F.softmax(scores, dim=-1)  #scaled dot product attention\n",
        "    return torch.matmul(attention_weights, values)"
      ],
      "metadata": {
        "id": "oXABaWLuyvmu"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def project_kv(key, value, E_proj):\n",
        "    \"\"\"\n",
        "    project value and key vectors from the original (n x d)- dimensional\n",
        "    \"\"\"\n",
        "    key = torch.einsum(\"bhjd, jk -> bhkd\", key, E_proj)\n",
        "    value = torch.einsum(\"bhjd, jk -> bhkd\", value, E_proj)\n",
        "    return key, value"
      ],
      "metadata": {
        "id": "AzwmdDgLDAMV"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def EF_proj(input, k, bias = True):\n",
        "    \"\"\"\n",
        "    helper function to init E and F projections\n",
        "    this variation, the E and F projections aren't trainable\n",
        "    \"\"\"\n",
        "    E_F = nn.Linear(input, k, bias) #randomly initialize the weights with N(0, 1)\n",
        "    torch.nn.init.normal_(E_F, mean=0.0, std=1/k)\n",
        "    return E_F\n"
      ],
      "metadata": {
        "id": "p4mlJMRfjflq"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LinformerSelfAttention(nn.Module):\n",
        "    '''\n",
        "    Linear self-attention\n",
        "    run time = O(nk) where n denotes sequence length and k denotes dim of linear projection of k\n",
        "    '''\n",
        "    def __init__(self, embed_dim, seq_len = 512, heads = 4, k = 128, dropout = .1):\n",
        "        assert (embed_dim % heads) == 0\n",
        "        self.heads = heads\n",
        "        self.k = k\n",
        "        \n",
        "        self.dim_head = embed_dim // heads   # each head is of size embed_dim / heads\n",
        "        self.seq_len = seq_len\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # create E and F projections \n",
        "        self.E_proj = EF_proj(embed_dim, k)  \n",
        "        self.F_proj = EF_proj(embed_dim, k)\n",
        "        \n",
        "        # linear layer that transforms embedding into q k and v vectors\n",
        "        self.to_q = nn.Linear(embed_dim, embed_dim, bias = False)\n",
        "        self.to_k = nn.Linear(embed_dim, embed_dim, bias = False)\n",
        "        self.to_v = nn.Linear(embed_dim, embed_dim, bias = False)\n",
        "\n",
        "        # resize matrix back to originnn.al input size\n",
        "        self.output_linear = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x, **kwargs):\n",
        "        assert x.dim() == 3 # batch size, length, k\n",
        "\n",
        "        # batch size, seq_len, dim, dim_head, num_heads, k\n",
        "        b, n, d, d_h, h, k = *x.shape, self.dim_head, self.heads, self.k\n",
        "        \n",
        "        # q,k, v vectors\n",
        "        query = self.to_q(x)\n",
        "        key = self.to_k(x)\n",
        "        value = self.to_v(x)\n",
        "\n",
        "        key, value = project_kv(key, value, self.E_proj)\n",
        "\n",
        "        out = scaled_attention(query,key, value)\n",
        "        \n",
        "        out = rearrange(out, \"b h i d -> b i (h d)\")\n",
        "        return self.output_linear(out)\n"
      ],
      "metadata": {
        "id": "59speaEuFQl_"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "    \"\"\"\n",
        "    A simple feed forward network with GELU activation function\n",
        "    \"\"\"\n",
        "    def __init__(self, in_dim,ff_dim, out_dim, dropout = .1):\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(in_dim, ff_dim)\n",
        "        self.linear2 = nn.Linear(ff_dim, out_dim)\n",
        "        self.gelu = nn.GELU()     # can change to activation of your choice, typically GELU in transformers\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.linear1(x)\n",
        "        x = self.gelu(x)\n",
        "        x = self.linear2(x)\n",
        "        x = self.dropout(x)\n",
        "\n"
      ],
      "metadata": {
        "id": "cKXaeFlUiogx"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Embeddings(nn.Module):\n",
        "    def __init__(self, seq_len, vocab_size, embed_dim):\n",
        "        super().__init__()\n",
        "        self.token_emb = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.pos_emb = nn.Embedding(seq_len, embed_dim)\n",
        "        self.layer_norm = nn.LayerNorm(embed_dim, eps = 1e-12)\n",
        "        self.dropout = nn.Dropout()\n",
        "    \n",
        "    def forward(self, input_ids):\n",
        "        # Create position IDs for input sequence\n",
        "        seq_length = input_ids.size(1)\n",
        "        position_ids = torch.arange(seq_length,\n",
        "        dtype=torch.long).unsqueeze(0)\n",
        "        # create token and position embeddings\n",
        "        token_emb = self.token_emb(input_ids)\n",
        "        pos_emb = self.position_embed(position_ids)\n",
        "        # Combine token and position embeddings\n",
        "        embeddings = token_emb + pos_emb\n",
        "        embeddings = self.layer_norm(embeddings)\n",
        "        embeddings = self.dropout(embeddings)\n",
        "        return embeddings"
      ],
      "metadata": {
        "id": "qvhiu585xMaB"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LinformerBlock(nn.Module):\n",
        "    def __init__(self,heads, dropout, embed_dim, in_dim, ff_dim, out_dim):\n",
        "        super().__init__()\n",
        "        self.layer_norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.layer_norm2 = nn.LayerNorm(embed_dim)\n",
        "        self.attention = LinformerSelfAttention(embed_dim = 768, seq_len = 512, heads = 4, k = 128, dropout = .1)\n",
        "        self.feed_forward = FeedForward(in_dim,ff_dim, out_dim, dropout = .1)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # pre normalization\n",
        "        hidden_state = self.layer_norm1(x)\n",
        "        # residual connection \n",
        "        x = x + self.attention(hidden_state)\n",
        "        # feed forward with residual\n",
        "        x = x + self.feed_forward((self.layer_norm2))\n",
        "        return x"
      ],
      "metadata": {
        "id": "RWba--YTAdfn"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MinLinFormer(pl.LightningModule):\n",
        "    \"\"\"\n",
        "    An encoder only min-linformer,  with positional embeddings from the original\n",
        "    Attention is all you need paper,\n",
        "    \n",
        "    \"\"\"\n",
        "    def __init__(self, embed_dim, heads, seq_len, vocab_size, in_dim, ff_dim, out_dim, dropout = .1,blocks = 2):\n",
        "        super().__init__()\n",
        "        self.embeddings = Embeddings(seq_len, vocab_size, embed_dim)\n",
        "        self.model = nn.Sequential(\n",
        "            self.embeddings,\n",
        "            *[LinformerBlock(heads = heads, dropout = dropout, embed_dim = embed_dim,\n",
        "            in_dim = in_dim, ff_dim = ff_dim, out_dim = out_dim) for _ in range(blocks)]\n",
        "            \n",
        "            )\n",
        "    def forward(self, x):\n",
        "        self.model(x) "
      ],
      "metadata": {
        "id": "lZmGablQq92K"
      },
      "execution_count": 32,
      "outputs": []
    }
  ]
}