{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "linformer.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMTTQKmT8V4cwU9l54SYOj5"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SHcMAJnk3pb7"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "from math import sqrt\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "!pip install pytorch_lightning\n",
        "import pytorch_lightning as pl\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install einops\n",
        "from einops import rearrange"
      ],
      "metadata": {
        "id": "oc8kfKV8sUn1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers.util.data import DataLoader\n",
        "# Data Module idea from George Hotz\n",
        "class BaseDataModule(pl.LightningDataModule):\n",
        "  def __init__(self, batch_size=32, split=0.8, *args, **kwargs):\n",
        "    super().__init__()\n",
        "    self.ds_X, self.ds_Y = self.get_dataset(*args, **kwargs)\n",
        "    shuffler = np.random.permutation(self.ds_X.shape[0])\n",
        "    self.ds_X = self.ds_X[shuffler]\n",
        "    self.ds_Y = self.ds_Y[shuffler]\n",
        "    self.split = int(self.ds_X.shape[0]*split)\n",
        "    self.batch_size = batch_size\n",
        "    \n",
        "  def train_dataloader(self):\n",
        "    ds_X_train, ds_Y_train = self.ds_X[0:self.split], self.ds_Y[0:self.split]\n",
        "    return DataLoader(list(zip(ds_X_train, ds_Y_train)), batch_size=self.batch_size)\n",
        "\n",
        "  def val_dataloader(self):\n",
        "    ds_X_test, ds_Y_test = self.ds_X[self.split:], self.ds_Y[self.split:]\n",
        "    return DataLoader(list(zip(ds_X_test, ds_Y_test)), batch_size=self.batch_size)\n",
        "\n",
        "  \n",
        "class WikipediaDataModule(BaseDataModule):\n",
        "  def get_dataset(self, seq_len=50):\n",
        "    global enwik8\n",
        "    if 'enwik8' not in globals():\n",
        "      import requests\n",
        "      enwik8_zipped = requests.get(\"https://data.deepai.org/enwik8.zip\").content\n",
        "      from zipfile import ZipFile\n",
        "      import io\n",
        "      enwik8 = ZipFile(io.BytesIO(enwik8_zipped)).read('enwik8')\n",
        "    en = np.frombuffer(enwik8, dtype=np.uint8).astype(np.int)\n",
        "    en = en[0:-seq_len+1]\n",
        "    en[en>127] = 127\n",
        "    return en[0:-1].reshape(-1, seq_len), en[1:].reshape(-1, seq_len)"
      ],
      "metadata": {
        "id": "kzhkU-LCtgYr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def scaled_attention(queries, keys, values):\n",
        "    d = queries.shape[-1]\n",
        "    scores = torch.matmul(queries, keys.transpose(-2,-1))/sqrt(d) #  dot product attention\n",
        "    attention_weights = F.softmax(scores, dim=-1)  #scaled dot product attention\n",
        "    return torch.matmul(attention_weights, values)"
      ],
      "metadata": {
        "id": "oXABaWLuyvmu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def project_kv(key, value, E_proj):\n",
        "    \"\"\"\n",
        "    project value and key vectors from the original (n x d)- dimensional\n",
        "    \"\"\"\n",
        "    key = torch.einsum(\"bhjd, jk -> bhkd\", key, E_proj)\n",
        "    value = torch.einsum(\"bhjd, jk -> bhkd\", value, E_proj)\n",
        "    return key, value"
      ],
      "metadata": {
        "id": "AzwmdDgLDAMV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def EF_proj(input, k, bias = True):\n",
        "    \"\"\"\n",
        "    helper function to init E and F projections\n",
        "    this variation, the E and F projections aren't trainable\n",
        "    \"\"\"\n",
        "    E_F = nn.Linear(input, k, bias) #randomly initialize the weights with N(0, 1)\n",
        "    torch.nn.init.normal_(E_F, mean=0.0, std=1/k)\n",
        "    return E_F\n"
      ],
      "metadata": {
        "id": "p4mlJMRfjflq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def EF_proj(input, k, bias = True):\n",
        "    \"\"\"\n",
        "    helper function to init E and F projections\n",
        "    this variation, the E and F projections aren't trainable\n",
        "    \"\"\"\n",
        "    E = torch.nn.Parameter(torch.randn(input))\n",
        "    return E"
      ],
      "metadata": {
        "id": "fUAZlQLEvmkr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LinformerSelfAttention(nn.Module):\n",
        "    '''\n",
        "    Linear self-attention\n",
        "    run time = O(nk) where n denotes sequence length and k denotes dim of linear projection of k\n",
        "    '''\n",
        "    def __init__(self, embed_dim, k, seq_len = 512, heads = 4,  dropout = .1):\n",
        "        super().__init__()\n",
        "        assert (embed_dim % heads) == 0\n",
        "        self.heads = heads\n",
        "        self.k = k\n",
        "        \n",
        "        self.dim_head = embed_dim // heads   # each head is of size embed_dim / heads\n",
        "        self.seq_len = seq_len\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # create E and F projections \n",
        "        self.E_proj = EF_proj(embed_dim, k)  \n",
        "        self.F_proj = EF_proj(embed_dim, k)\n",
        "        \n",
        "        # linear layer that transforms embedding into q k and v vectors\n",
        "        self.to_q = nn.Linear(embed_dim, embed_dim, bias = False)\n",
        "        self.to_k = nn.Linear(embed_dim, embed_dim, bias = False)\n",
        "        self.to_v = nn.Linear(embed_dim, embed_dim, bias = False)\n",
        "\n",
        "        # resize matrix back to originnn.al input size\n",
        "        self.output_linear = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x, **kwargs):\n",
        "        assert x.dim() == 3 # batch size, length, k\n",
        "\n",
        "        # batch size, seq_len, dim, dim_head, num_heads, k\n",
        "        b, n, d, d_h, h, k = *x.shape, self.dim_head, self.heads, self.k\n",
        "        \n",
        "        # q,k, v vectors\n",
        "        query = self.to_q(x)\n",
        "        key = self.to_k(x)\n",
        "        value = self.to_v(x)\n",
        "\n",
        "        key, value = project_kv(key, value, self.E_proj)\n",
        "\n",
        "        out = scaled_attention(query,key, value)\n",
        "        \n",
        "\n",
        "        return self.output_linear(out)\n"
      ],
      "metadata": {
        "id": "59speaEuFQl_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LinformerSelfAttention(nn.Module):\n",
        "    def __init__(self, embed_dim, k, seq_len, dim_head, heads,dropout = .1):\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.k = k\n",
        "        self.seq_len = seq_len\n",
        "        self.dim_head = dim_head if dim_head is not None else embed_dim // heads\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.E_proj = EF_proj(seq_len, k)  \n",
        "        self.F_proj = EF_proj(seq_len, k)\n",
        "        self.output_linear = nn.Linear(dim_head * heads, embed_dim)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        b, n, d, d_h, h, k = *x.shape, self.dim_head, self.heads, self.k\n",
        "        \n",
        "\n"
      ],
      "metadata": {
        "id": "CR2caE1bp2os"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "    \"\"\"\n",
        "    A simple feed forward network with GELU activation function\n",
        "    \"\"\"\n",
        "    def __init__(self, in_dim,ff_dim, out_dim, dropout = .1):\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(in_dim, ff_dim)\n",
        "        self.linear2 = nn.Linear(ff_dim, out_dim)\n",
        "        self.gelu = nn.GELU()     # can change to activation of your choice, typically GELU in transformers\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.linear1(x)\n",
        "        x = self.gelu(x)\n",
        "        x = self.linear2(x)\n",
        "        x = self.dropout(x)\n",
        "\n"
      ],
      "metadata": {
        "id": "cKXaeFlUiogx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Embeddings(nn.Module):\n",
        "    def __init__(self, seq_len, vocab_size, embed_dim):\n",
        "        super().__init__()\n",
        "        self.token_emb = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.pos_emb = nn.Embedding(seq_len, embed_dim)\n",
        "        self.layer_norm = nn.LayerNorm(embed_dim, eps = 1e-12)\n",
        "        self.dropout = nn.Dropout()\n",
        "    \n",
        "    def forward(self, input_ids):\n",
        "        # Create position IDs for input sequence\n",
        "        seq_length = input_ids.size(1)\n",
        "        position_ids = torch.arange(seq_length,\n",
        "        dtype=torch.long).unsqueeze(0)\n",
        "        # create token and position embeddings\n",
        "        token_emb = self.token_emb(input_ids)\n",
        "        pos_emb = self.position_embed(position_ids)\n",
        "        # Combine token and position embeddings\n",
        "        embeddings = token_emb + pos_emb\n",
        "        embeddings = self.layer_norm(embeddings)\n",
        "        embeddings = self.dropout(embeddings)\n",
        "        return embeddings"
      ],
      "metadata": {
        "id": "qvhiu585xMaB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Embeddings(nn.Module):\n",
        "  def __init__(self, seq_len, vocab_size, embed_dim):\n",
        "    super().__init__()\n",
        "    self.token_emb = nn.Embedding(vocab_size, embed_dim)\n",
        "    self.pos_emb = nn.Embedding(seq_len, embed_dim)\n",
        "  def forward(self, x):\n",
        "    pos = torch.arange(0, x.size(1), dtype=torch.int32, device=x.device)\n",
        "    return self.token_emb(x) + self.pos_emb(pos).view(1, x.size(1), -1)"
      ],
      "metadata": {
        "id": "wtNedHVMwi1V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LinformerBlock(nn.Module):\n",
        "    def __init__(self,heads, dropout, embed_dim, in_dim, ff_dim, out_dim):\n",
        "        super().__init__()\n",
        "        self.layer_norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.layer_norm2 = nn.LayerNorm(embed_dim)\n",
        "        self.attention = LinformerSelfAttention(embed_dim = 768, seq_len = 512, heads = 4, k = 128, dropout = .1)\n",
        "        self.feed_forward = FeedForward(in_dim,ff_dim, out_dim, dropout = .1)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # pre normalization\n",
        "        hidden_state = self.layer_norm1(x)\n",
        "        # residual connection \n",
        "        x = x + self.attention(hidden_state)\n",
        "        # feed forward with residual\n",
        "        x = x + self.feed_forward((self.layer_norm2))\n",
        "        return x"
      ],
      "metadata": {
        "id": "RWba--YTAdfn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MinLinFormer(pl.LightningModule):\n",
        "    \"\"\"\n",
        "    An encoder only min-linformer,  with positional embeddings from the original\n",
        "    Attention is all you need paper,\n",
        "    \n",
        "    \"\"\"\n",
        "    def __init__(self, k, embed_dim, heads, seq_len, vocab_size, in_dim, ff_dim, out_dim,  dropout = .1,blocks = 2):\n",
        "        super().__init__()\n",
        "        self.embeddings = Embeddings(seq_len, vocab_size, embed_dim)\n",
        "        self.model = nn.Sequential(\n",
        "            self.embeddings,\n",
        "            *[LinformerBlock(heads = heads, dropout = dropout, embed_dim = embed_dim,\n",
        "            in_dim = in_dim, ff_dim = ff_dim, out_dim = out_dim) for _ in range(blocks)]\n",
        "            \n",
        "            )\n",
        "    def forward(self, x):\n",
        "        self.model(x) \n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        output = self.model(x)\n",
        "        loss = F.nll_loss(output.view(-1, self.max_value), y.view(-1))\n",
        "        self.log(\"train_loss\", loss)\n",
        "        return loss\n",
        "  \n",
        "    def validation_step(self, val_batch, batch_idx):\n",
        "        x, y = val_batch\n",
        "        pred = self.model(x).argmax(dim=2)\n",
        "        val_accuracy = (pred == y).type(torch.float).mean()\n",
        "        self.log(\"val_accuracy\", val_accuracy, prog_bar=True)\n",
        "  \n",
        "    def configure_optimizers(self):\n",
        "    \n",
        "        return torch.optim.Adam(self.parameters(), lr=3e-4)"
      ],
      "metadata": {
        "id": "lZmGablQq92K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MinLinFormer(embed_dim = 512, heads = 2, seq_len = 5, vocab_size = 10000, in_dim = 512, ff_dim = 1024, out_dim  = 512)\n",
        "trainer = pl.Trainer(enable_progress_bar=True, max_epochs=5, gpus=1)\n",
        "data = AdditionDataModule(batch_size=64)\n",
        "\n",
        "trainer.fit(model, data)"
      ],
      "metadata": {
        "id": "cronBGQpt4v1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LinformerLM(nn.Module):\n",
        "    def __init__(self, num_tokens, dim, seq_len, depth, k = 256, heads = 4, dropout = 0.):\n",
        "        super().__init__()\n",
        "        self.token_emb = nn.Embedding(num_tokens, dim)\n",
        "        self.pos_emb = nn.Embedding(seq_len, dim)\n",
        "        self.linformer = MinLinFormer(dim, seq_len, depth, k = k, heads = heads, dim_head = dim_head,\n",
        "                one_kv_head = one_kv_head, share_kv = share_kv, reversible = reversible, dropout = dropout)\n",
        "        self.to_logits = nn.Linear(dim, num_tokens)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.token_emb(x)\n",
        "        x = self.pos_emb(torch.arange(x.shape[1], device=x.device)) + x\n",
        "        x = self.linformer(x)\n",
        "        out = self.to_logits(x)\n",
        "        return out"
      ],
      "metadata": {
        "id": "t9eO5Ue0xSz3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}