{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "linformer.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO80gCKsZN/JownrxbL/I2i"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SHcMAJnk3pb7"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "from math import sqrt\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "!pip install pytorch_lightning\n",
        "import pytorch_lightning as pl\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install einops\n",
        "from einops import rearrange"
      ],
      "metadata": {
        "id": "oc8kfKV8sUn1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "# Data Module idea from George Hotz\n",
        "class BaseDataModule(pl.LightningDataModule):\n",
        "  def __init__(self, batch_size=32, split=0.8, *args, **kwargs):\n",
        "    super().__init__()\n",
        "    self.ds_X, self.ds_Y = self.get_dataset(*args, **kwargs)\n",
        "    shuffler = np.random.permutation(self.ds_X.shape[0])\n",
        "    self.ds_X = self.ds_X[shuffler]\n",
        "    self.ds_Y = self.ds_Y[shuffler]\n",
        "    self.split = int(self.ds_X.shape[0]*split)\n",
        "    self.batch_size = batch_size\n",
        "    \n",
        "  def train_dataloader(self):\n",
        "    ds_X_train, ds_Y_train = self.ds_X[0:self.split], self.ds_Y[0:self.split]\n",
        "    return DataLoader(list(zip(ds_X_train, ds_Y_train)), batch_size=self.batch_size)\n",
        "\n",
        "  def val_dataloader(self):\n",
        "    ds_X_test, ds_Y_test = self.ds_X[self.split:], self.ds_Y[self.split:]\n",
        "    return DataLoader(list(zip(ds_X_test, ds_Y_test)), batch_size=self.batch_size)\n",
        "\n",
        "  \n",
        "class WikipediaDataModule(BaseDataModule):\n",
        "  def get_dataset(self, seq_len=50):\n",
        "    global enwik8\n",
        "    if 'enwik8' not in globals():\n",
        "      import requests\n",
        "      enwik8_zipped = requests.get(\"https://data.deepai.org/enwik8.zip\").content\n",
        "      from zipfile import ZipFile\n",
        "      import io\n",
        "      enwik8 = ZipFile(io.BytesIO(enwik8_zipped)).read('enwik8')\n",
        "    en = np.frombuffer(enwik8, dtype=np.uint8).astype(np.int)\n",
        "    en = en[0:-seq_len+1]\n",
        "    en[en>127] = 127\n",
        "    return en[0:-1].reshape(-1, seq_len), en[1:].reshape(-1, seq_len)"
      ],
      "metadata": {
        "id": "kzhkU-LCtgYr"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, embed_dim, hidden_dim, dropout):\n",
        "        \"\"\"\n",
        "        A feed forward network after scaled dot product attention\n",
        "        Params:\n",
        "        embed_dim = embedding dimension of vector\n",
        "        hidden_dim = hidden dimension in the FF network, generally 2-4x larger than embed_dim\n",
        "        dropout = % dropout for training\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.FFN = nn.Sequential(\n",
        "            nn.Linear(embed_dim, hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(hidden_dim, embed_dim),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        x = self.FFN(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "oXABaWLuyvmu"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def attention(query, key, value):\n",
        "    \"\"\"\n",
        "    calculate scaled dot product attention given a q, k and v\n",
        "    Params:\n",
        "    query -> a given query tensor\n",
        "    key -> a given key tensor\n",
        "    value -> a given value tensor\n",
        "    \"\"\"\n",
        "    dim = query.shape[-1]\n",
        "    # (Query * tranpose(key)) / sqrt(dim)\n",
        "    scores = torch.bmm(query, key.transpose(-2, -1)) / sqrt(dim)\n",
        "    weights = F.softmax(scores, dim = -1)\n",
        "    return torch.bmm(weights, value)"
      ],
      "metadata": {
        "id": "IpSAH07RGJgl"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_EF(input_dim, k, bias = True):\n",
        "    \"\"\"\n",
        "    helper function to init E and F projections\n",
        "    this variation, the E and F projections aren't trainable\n",
        "    \"\"\"\n",
        "    E_F = nn.Linear(input_dim, k, bias)\n",
        "    torch.nn.init.xavier_normal_(E_F.weight)\n",
        "    return E_F\n",
        " "
      ],
      "metadata": {
        "id": "p4mlJMRfjflq"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionHead(nn.Module):\n",
        "    def __init__(self, embed_dim, head_dim, k, E_i, F_i = None, share_kv = False):\n",
        "        super().__init__()\n",
        "        self.to_q = nn.Linear(embed_dim, head_dim)\n",
        "        self.to_k = nn.Linear(embed_dim, head_dim)   \n",
        "        self.to_v = nn.Linear(embed_dim, head_dim)\n",
        "        self.E_i = E_i \n",
        "        self.F_i = F_i\n",
        "        self.share_kv = share_kv\n",
        "    \n",
        "    def forward(self, x):\n",
        "        if self.share_kv == True:\n",
        "            #down project k and v vectors\n",
        "            down_k = torch.matmul(self.E_i, self.to_k(x)) \n",
        "            down_v = torch.matmul(self.E_i, self.to_v(x))\n",
        "        else:\n",
        "            down_k = torch.matmul(self.E_i, self.to_k(x)) \n",
        "            down_v = torch.matmul(self.F_i, self.to_v(x))\n",
        "        \n",
        "        attn = attention(self.to_q(x), down_k, down_v)\n",
        "        return attn\n"
      ],
      "metadata": {
        "id": "otjNqs_802-m"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Calculates the Multi-Headed attention\n",
        "    Params:\n",
        "    num_heads -> number of heads to use, each head_dim is calculated as embed_dim // num_heads \n",
        "    embed_dim -> embedding dimension of vector x\n",
        "    \"\"\"\n",
        "    def __init__(self, num_heads, embed_dim, k, share_kv = False, share_headwise = True):\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_dim // num_heads \n",
        "\n",
        "        self.E_i = get_EF(embed_dim, k)\n",
        "        self.F_i = get_EF(embed_dim, k)\n",
        "        \n",
        "        if share_headwise == False:\n",
        "            self.heads = nn.ModuleList(\n",
        "            AttentionHead(self.embed_dim, self.head_dim, k, get_EF(embed_dim,k), get_EF(embed_dim, k), share_kv) \n",
        "            for _ in range(num_heads)\n",
        "        )\n",
        "        else:\n",
        "            self.heads = nn.ModuleList(\n",
        "                AttentionHead(self.embed_dim, self.head_dim, k, self.E_i, self.F_i, share_kv) \n",
        "                for _ in range(num_heads)\n",
        "            )\n",
        "        self.to_out = nn.Linear(embed_dim, embed_dim)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # calculate attention for each head and concatenate tensor\n",
        "        x = torch.cat([head(x) for head in self.heads], dim = -1)\n",
        "        x = self.to_out(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "PNUztBNN46FC"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Embeddings(nn.Module):\n",
        "  def __init__(self, seq_len, vocab_size, embed_dim):\n",
        "    super().__init__()\n",
        "    self.token_emb = nn.Embedding(vocab_size, embed_dim)\n",
        "    self.pos_emb = nn.Embedding(seq_len, embed_dim)\n",
        "  def forward(self, x):\n",
        "    pos = torch.arange(0, x.size(1), dtype=torch.int32, device=x.device)\n",
        "    return self.token_emb(x) + self.pos_emb(pos).view(1, x.size(1), -1)"
      ],
      "metadata": {
        "id": "wtNedHVMwi1V"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LinformerBlock(nn.Module):\n",
        "    def __init__(self,heads, dropout, embed_dim, hidden_dim , k, share_kv, share_headwise):\n",
        "        super().__init__()\n",
        "        self.layer_norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.layer_norm2 = nn.LayerNorm(embed_dim)\n",
        "        self.attention = MultiHeadAttention(num_heads = heads, embed_dim =embed_dim, k = k, share_kv = False, share_headwise = True)\n",
        "        self.feed_forward = FeedForward(embed_dim,hidden_dim, dropout = dropout)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # pre normalization\n",
        "        hidden_state = self.layer_norm1(x)\n",
        "        # residual connection \n",
        "        x = x + self.attention(hidden_state)\n",
        "        # feed forward with residual\n",
        "        x = x + self.feed_forward((self.layer_norm2))\n",
        "        return x"
      ],
      "metadata": {
        "id": "RWba--YTAdfn"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MinLinFormer(pl.LightningModule):\n",
        "    \"\"\"\n",
        "    An encoder only min-linformer,  with positional embeddings from the original\n",
        "    Attention is all you need paper,\n",
        "    \n",
        "    \"\"\"\n",
        "    def __init__(self, k, embed_dim, heads, seq_len, vocab_size, hidden_dim , share_kv, share_headwise, dropout ,blocks ):\n",
        "        super().__init__()\n",
        "        self.embeddings = Embeddings(seq_len, vocab_size, embed_dim)\n",
        "        self.model = nn.Sequential(\n",
        "            self.embeddings,\n",
        "            *[LinformerBlock(heads = heads, dropout = dropout, embed_dim = embed_dim,\n",
        "            hidden_dim = hidden_dim, k = k, share_kv = share_kv,\n",
        "            share_headwise = share_headwise) for _ in range(blocks)]\n",
        "            \n",
        "            )\n",
        "    def forward(self, x):\n",
        "        self.model(x) \n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        output = self.model(x)\n",
        "        loss = F.nll_loss(output.view(-1, self.max_value), y.view(-1))\n",
        "        self.log(\"train_loss\", loss)\n",
        "        return loss\n",
        "  \n",
        "    def validation_step(self, val_batch, batch_idx):\n",
        "        x, y = val_batch\n",
        "        pred = self.model(x).argmax(dim=2)\n",
        "        val_accuracy = (pred == y).type(torch.float).mean()\n",
        "        self.log(\"val_accuracy\", val_accuracy, prog_bar=True)\n",
        "  \n",
        "    def configure_optimizers(self):\n",
        "    \n",
        "        return torch.optim.Adam(self.parameters(), lr=3e-4)"
      ],
      "metadata": {
        "id": "lZmGablQq92K"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MinLinFormer(embed_dim = 512, heads = 2, k = 128, seq_len = 5, \n",
        "                     vocab_size = 10000, hidden_dim = 1024,\n",
        "                     blocks = 2, dropout = .1, share_headwise=True, share_kv = True)\n",
        "trainer = pl.Trainer(enable_progress_bar=True, max_epochs=5, gpus=0)\n",
        "data = WikipediaDataModule(batch_size=64)\n",
        "\n",
        "trainer.fit(model, data)"
      ],
      "metadata": {
        "id": "cronBGQpt4v1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LinformerLM(nn.Module):\n",
        "    def __init__(self, num_tokens, dim, seq_len, depth, k = 256, heads = 4, dropout = 0.):\n",
        "        super().__init__()\n",
        "        self.token_emb = nn.Embedding(num_tokens, dim)\n",
        "        self.pos_emb = nn.Embedding(seq_len, dim)\n",
        "        self.linformer = MinLinFormer(dim, seq_len, depth, k = k, heads = heads, dim_head = dim_head,\n",
        "                one_kv_head = one_kv_head, share_kv = share_kv, reversible = reversible, dropout = dropout)\n",
        "        self.to_logits = nn.Linear(dim, num_tokens)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.token_emb(x)\n",
        "        x = self.pos_emb(torch.arange(x.shape[1], device=x.device)) + x\n",
        "        x = self.linformer(x)\n",
        "        out = self.to_logits(x)\n",
        "        return out"
      ],
      "metadata": {
        "id": "t9eO5Ue0xSz3"
      },
      "execution_count": 86,
      "outputs": []
    }
  ]
}